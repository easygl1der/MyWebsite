---
UID: 20241008193959 
aliases: 
tags: 
source: 
cssclass: 
created: 2024-10-08
---

## ✍内容
Say $\displaystyle Y_{n}\to Y$ in proability if for all $\displaystyle \epsilon>0$, $\displaystyle P(\lvert Y_{n}-Y \rvert>\epsilon)\to0$ as $\displaystyle n\to \infty$.
A family of random variables $\displaystyle X_{i}, i\in I$ with $\displaystyle EX_{i}^{2}<\infty$ is said to be **uncorrelated** if
$$
E(X_{i}X_{j})=EX_{i}EX_{j},\qquad \text{whenevr } i\neq j
$$
If $\displaystyle X_{i}$ are uncorrelated, then $\displaystyle var\left( \sum X_{i} \right)=\sum var(X_{i})$.
If $\displaystyle p>0$ and $\displaystyle E\lvert Z_{n} \rvert^{p}\to0$ then $\displaystyle Z_{n}\to0$ in probability.
$\displaystyle L^{2}$ weak law: let $\displaystyle X_{1}, X_{2},\dots$ uncorrealted random variables with $\displaystyle EX_{i}=\mu,var(X_{i})<C<\infty$. If $\displaystyle S_{n}=X_{1}+\dots+X_{n}$ then as $\displaystyle n\to \infty$, $\displaystyle S_{n}/n\to \mu$ in $\displaystyle L^{2}$ and in probability. (The proof is trivial.)
The case that $\displaystyle X_{1},X_{2},\dots$ are independent random variables that all have the same distribution. In the jargon, they are **independent and identically distributed** or **i.i.d.** for short.


